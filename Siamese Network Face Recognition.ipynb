{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_olivetti_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = fetch_olivetti_faces()\n",
    "targets, data = faces[\"target\"], faces[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 64\n",
    "data = data.reshape(-1, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    data, \n",
    "    targets, \n",
    "    stratify = targets,\n",
    "    test_size = 0.2,\n",
    "    random_state = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = x_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_triplets(examples):\n",
    "    n = examples[0].shape[1]\n",
    "    plt.figure(figsize = (6, 2))\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, 1 + i)\n",
    "        plt.imshow(examples[i].reshape(n, n), cmap = 'gray')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(x_train, y_train, batch_size):\n",
    "    m = x_train.shape[0]\n",
    "    n = x_train.shape[1]\n",
    "    \n",
    "    x_anchors = np.zeros((batch_size, n, n))\n",
    "    x_positives = np.zeros((batch_size, n, n))\n",
    "    x_negatives = np.zeros((batch_size, n, n))\n",
    "    \n",
    "    for i in range(0, batch_size):\n",
    "        # We need to find an anchor, a positive example and a negative example\n",
    "        random_index = random.randint(0, m - 1)\n",
    "        x_anchor = x_train[random_index]\n",
    "        y = y_train[random_index]\n",
    "        \n",
    "        indices_for_pos = np.squeeze(np.where(y_train == y))\n",
    "        indices_for_neg = np.squeeze(np.where(y_train != y))\n",
    "           \n",
    "        x_positive = x_train[indices_for_pos[random.randint(0, len(indices_for_pos) - 1)]]\n",
    "        x_negative = x_train[indices_for_neg[random.randint(0, len(indices_for_neg) - 1)]]\n",
    "        \n",
    "        x_anchors[i] = x_anchor\n",
    "        x_positives[i] = x_positive\n",
    "        x_negatives[i] = x_negative\n",
    "        \n",
    "    x_anchors = x_anchors.reshape(-1, n, n, 1)    \n",
    "    x_positives = x_positives.reshape(-1, n, n, 1)\n",
    "    x_negatives = x_negatives.reshape(-1, n, n, 1)\n",
    "    \n",
    "    return x_anchors, x_positives, x_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 64, 64, 16)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 64, 64, 16)        64        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 64, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 32, 32, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 32, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                262208    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                1040      \n",
      "=================================================================\n",
      "Total params: 295,440\n",
      "Trainable params: 295,088\n",
      "Non-trainable params: 352\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "emb_size = 16\n",
    "\n",
    "# Initialising the CNN\n",
    "embedding_model = Sequential()\n",
    "\n",
    "# 1 - Convolution\n",
    "embedding_model.add(Conv2D(16,(3,3), padding='same', input_shape=((n, n, 1))))\n",
    "embedding_model.add(BatchNormalization())\n",
    "embedding_model.add(Activation('relu'))\n",
    "embedding_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "embedding_model.add(Dropout(0.25))\n",
    "\n",
    "# 2nd Convolution layer\n",
    "embedding_model.add(Conv2D(32,(5,5), padding='same'))\n",
    "embedding_model.add(BatchNormalization())\n",
    "embedding_model.add(Activation('relu'))\n",
    "embedding_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "embedding_model.add(Dropout(0.25))\n",
    "\n",
    "# 3rd Convolution layer\n",
    "embedding_model.add(Conv2D(64,(3,3), padding='same'))\n",
    "embedding_model.add(BatchNormalization())\n",
    "embedding_model.add(Activation('relu'))\n",
    "embedding_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "embedding_model.add(Dropout(0.25))\n",
    "\n",
    "# Flattening\n",
    "embedding_model.add(Flatten())\n",
    "\n",
    "# Fully connected layer\n",
    "embedding_model.add(Dense(64))\n",
    "embedding_model.add(BatchNormalization())\n",
    "embedding_model.add(Activation('relu'))\n",
    "embedding_model.add(Dropout(0.25))\n",
    "\n",
    "# Last layer\n",
    "embedding_model.add(Dense(emb_size, activation='sigmoid'))\n",
    "\n",
    "embedding_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 64, 64, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 64, 64, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 64, 64, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 16)           295440      input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 48)           0           sequential[0][0]                 \n",
      "                                                                 sequential[1][0]                 \n",
      "                                                                 sequential[2][0]                 \n",
      "==================================================================================================\n",
      "Total params: 295,440\n",
      "Trainable params: 295,088\n",
      "Non-trainable params: 352\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_anchor = tf.keras.layers.Input(shape = (n, n, 1))\n",
    "input_positive = tf.keras.layers.Input(shape = (n, n, 1))\n",
    "input_negative = tf.keras.layers.Input(shape = (n, n, 1))\n",
    "\n",
    "embedding_anchor = embedding_model(input_anchor)\n",
    "embedding_positive = embedding_model(input_positive)\n",
    "embedding_negative = embedding_model(input_negative)\n",
    "\n",
    "output = tf.keras.layers.concatenate([embedding_anchor, embedding_positive, embedding_negative], axis=1)\n",
    "\n",
    "net = tf.keras.models.Model([input_anchor, input_positive, input_negative], output)\n",
    "net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(alpha, emb_size):\n",
    "    \n",
    "    def loss(y_true, y_pred): # Euclidean distance\n",
    "        anchor, positive, negative = y_pred[:,:emb_size], y_pred[:,emb_size:2*emb_size], y_pred[:,2*emb_size:]\n",
    "        positive_dist = tf.reduce_mean(tf.square(anchor - positive), axis = 1)\n",
    "        negative_dist = tf.reduce_mean(tf.square(anchor - negative), axis = 1)\n",
    "        \n",
    "        return tf.maximum(positive_dist - negative_dist + alpha, 0.)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(x_train, y_train, batch_size, emb_size):\n",
    "    while True:\n",
    "        x = create_batch(x_train, y_train, batch_size)\n",
    "        y = np.zeros((batch_size, 3*emb_size))\n",
    "        # Target (y) is a matrix of 0s. We aren't actually using it.\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "20/20 [==============================] - 2s 111ms/step - loss: 0.1509\n",
      "Epoch 2/100\n",
      "20/20 [==============================] - 2s 111ms/step - loss: 0.1089\n",
      "Epoch 3/100\n",
      "20/20 [==============================] - 2s 111ms/step - loss: 0.0936\n",
      "Epoch 4/100\n",
      "20/20 [==============================] - 2s 113ms/step - loss: 0.0818\n",
      "Epoch 5/100\n",
      "20/20 [==============================] - 3s 127ms/step - loss: 0.0801\n",
      "Epoch 6/100\n",
      "20/20 [==============================] - 3s 128ms/step - loss: 0.0690\n",
      "Epoch 7/100\n",
      "20/20 [==============================] - 2s 119ms/step - loss: 0.0599\n",
      "Epoch 8/100\n",
      "20/20 [==============================] - 2s 119ms/step - loss: 0.0612\n",
      "Epoch 9/100\n",
      "20/20 [==============================] - 2s 119ms/step - loss: 0.0555\n",
      "Epoch 10/100\n",
      "20/20 [==============================] - 2s 119ms/step - loss: 0.0482\n",
      "Epoch 11/100\n",
      "20/20 [==============================] - 2s 119ms/step - loss: 0.0549\n",
      "Epoch 12/100\n",
      "20/20 [==============================] - 2s 119ms/step - loss: 0.0445\n",
      "Epoch 13/100\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0493\n",
      "Epoch 14/100\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0481\n",
      "Epoch 15/100\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0391\n",
      "Epoch 16/100\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0402\n",
      "Epoch 17/100\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0407\n",
      "Epoch 18/100\n",
      "20/20 [==============================] - 3s 126ms/step - loss: 0.0378\n",
      "Epoch 19/100\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0370\n",
      "Epoch 20/100\n",
      "20/20 [==============================] - 2s 122ms/step - loss: 0.0315\n",
      "Epoch 21/100\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0313\n",
      "Epoch 22/100\n",
      "20/20 [==============================] - 2s 119ms/step - loss: 0.0340\n",
      "Epoch 23/100\n",
      "20/20 [==============================] - 2s 119ms/step - loss: 0.0334\n",
      "Epoch 24/100\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0268\n",
      "Epoch 25/100\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0302\n",
      "Epoch 26/100\n",
      "20/20 [==============================] - 3s 127ms/step - loss: 0.0304\n",
      "Epoch 27/100\n",
      "20/20 [==============================] - 3s 145ms/step - loss: 0.0261\n",
      "Epoch 28/100\n",
      "20/20 [==============================] - 2s 117ms/step - loss: 0.0315\n",
      "Epoch 29/100\n",
      "20/20 [==============================] - 3s 126ms/step - loss: 0.0277\n",
      "Epoch 30/100\n",
      "20/20 [==============================] - 2s 116ms/step - loss: 0.0250\n",
      "Epoch 31/100\n",
      "20/20 [==============================] - 2s 114ms/step - loss: 0.0338\n",
      "Epoch 32/100\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0294\n",
      "Epoch 33/100\n",
      "20/20 [==============================] - 2s 124ms/step - loss: 0.0277\n",
      "Epoch 34/100\n",
      "20/20 [==============================] - 2s 119ms/step - loss: 0.0264\n",
      "Epoch 35/100\n",
      "20/20 [==============================] - 2s 119ms/step - loss: 0.0262\n",
      "Epoch 36/100\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0248\n",
      "Epoch 37/100\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0246\n",
      "Epoch 38/100\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0235\n",
      "Epoch 39/100\n",
      "20/20 [==============================] - 3s 128ms/step - loss: 0.0222\n",
      "Epoch 40/100\n",
      "20/20 [==============================] - 2s 122ms/step - loss: 0.0180\n",
      "Epoch 41/100\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0271\n",
      "Epoch 42/100\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0246\n",
      "Epoch 43/100\n",
      "20/20 [==============================] - 2s 123ms/step - loss: 0.0251\n",
      "Epoch 44/100\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0204\n",
      "Epoch 45/100\n",
      "20/20 [==============================] - 2s 119ms/step - loss: 0.0252\n",
      "Epoch 46/100\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0200\n",
      "Epoch 47/100\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0319\n",
      "Epoch 48/100\n",
      "20/20 [==============================] - 2s 124ms/step - loss: 0.0196\n",
      "Epoch 49/100\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0257\n",
      "Epoch 50/100\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0215\n",
      "Epoch 51/100\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0186\n",
      "Epoch 52/100\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0212\n",
      "Epoch 53/100\n",
      "20/20 [==============================] - 3s 130ms/step - loss: 0.0176\n",
      "Epoch 54/100\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0215\n",
      "Epoch 55/100\n",
      "20/20 [==============================] - 2s 124ms/step - loss: 0.0189\n",
      "Epoch 56/100\n",
      "20/20 [==============================] - 2s 124ms/step - loss: 0.0204\n",
      "Epoch 57/100\n",
      "20/20 [==============================] - 2s 124ms/step - loss: 0.0219\n",
      "Epoch 58/100\n",
      "20/20 [==============================] - 2s 122ms/step - loss: 0.0218\n",
      "Epoch 59/100\n",
      "20/20 [==============================] - 2s 124ms/step - loss: 0.0183\n",
      "Epoch 60/100\n",
      "20/20 [==============================] - 3s 129ms/step - loss: 0.0230\n",
      "Epoch 61/100\n",
      "20/20 [==============================] - 2s 122ms/step - loss: 0.0197\n",
      "Epoch 62/100\n",
      "20/20 [==============================] - 2s 123ms/step - loss: 0.0165\n",
      "Epoch 63/100\n",
      "20/20 [==============================] - 2s 123ms/step - loss: 0.0232\n",
      "Epoch 64/100\n",
      "20/20 [==============================] - 2s 122ms/step - loss: 0.0138\n",
      "Epoch 65/100\n",
      "20/20 [==============================] - 2s 123ms/step - loss: 0.0166\n",
      "Epoch 66/100\n",
      "20/20 [==============================] - 2s 125ms/step - loss: 0.0224\n",
      "Epoch 67/100\n",
      "20/20 [==============================] - 2s 122ms/step - loss: 0.0206\n",
      "Epoch 68/100\n",
      "20/20 [==============================] - 3s 143ms/step - loss: 0.0164\n",
      "Epoch 69/100\n",
      "20/20 [==============================] - 2s 123ms/step - loss: 0.0229\n",
      "Epoch 70/100\n",
      "20/20 [==============================] - 3s 151ms/step - loss: 0.0260\n",
      "Epoch 71/100\n",
      "20/20 [==============================] - 3s 147ms/step - loss: 0.0169\n",
      "Epoch 72/100\n",
      "20/20 [==============================] - 2s 122ms/step - loss: 0.0184\n",
      "Epoch 73/100\n",
      "20/20 [==============================] - 2s 119ms/step - loss: 0.0193\n",
      "Epoch 74/100\n",
      "20/20 [==============================] - 3s 141ms/step - loss: 0.0145\n",
      "Epoch 75/100\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 0.0176\n",
      "Epoch 76/100\n",
      "20/20 [==============================] - 3s 126ms/step - loss: 0.0155\n",
      "Epoch 77/100\n",
      "20/20 [==============================] - 2s 124ms/step - loss: 0.0160\n",
      "Epoch 78/100\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0195\n",
      "Epoch 79/100\n",
      "20/20 [==============================] - 2s 123ms/step - loss: 0.0201\n",
      "Epoch 80/100\n",
      "20/20 [==============================] - 3s 134ms/step - loss: 0.0174\n",
      "Epoch 81/100\n",
      "20/20 [==============================] - 3s 127ms/step - loss: 0.0121\n",
      "Epoch 82/100\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.0164\n",
      "Epoch 83/100\n",
      "20/20 [==============================] - 2s 122ms/step - loss: 0.0144\n",
      "Epoch 84/100\n",
      "20/20 [==============================] - 2s 122ms/step - loss: 0.0172\n",
      "Epoch 85/100\n",
      "20/20 [==============================] - 2s 124ms/step - loss: 0.0172\n",
      "Epoch 86/100\n",
      "20/20 [==============================] - 3s 126ms/step - loss: 0.0154\n",
      "Epoch 87/100\n",
      "20/20 [==============================] - 3s 128ms/step - loss: 0.0150\n",
      "Epoch 88/100\n",
      "20/20 [==============================] - 3s 126ms/step - loss: 0.0188\n",
      "Epoch 89/100\n",
      "20/20 [==============================] - 2s 123ms/step - loss: 0.0184\n",
      "Epoch 90/100\n",
      "20/20 [==============================] - 2s 123ms/step - loss: 0.0119\n",
      "Epoch 91/100\n",
      "20/20 [==============================] - 2s 122ms/step - loss: 0.0144\n",
      "Epoch 92/100\n",
      "20/20 [==============================] - 2s 122ms/step - loss: 0.0148\n",
      "Epoch 93/100\n",
      "20/20 [==============================] - 2s 123ms/step - loss: 0.0154\n",
      "Epoch 94/100\n",
      "20/20 [==============================] - 2s 123ms/step - loss: 0.0178\n",
      "Epoch 95/100\n",
      "20/20 [==============================] - 3s 126ms/step - loss: 0.0144\n",
      "Epoch 96/100\n",
      "20/20 [==============================] - 2s 122ms/step - loss: 0.0165\n",
      "Epoch 97/100\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0151\n",
      "Epoch 98/100\n",
      "20/20 [==============================] - 2s 122ms/step - loss: 0.0187\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0146\n",
      "Epoch 100/100\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0152\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "epochs = 100\n",
    "steps_per_epoch = int(m/batch_size)\n",
    "\n",
    "net.compile(\n",
    "    loss = triplet_loss(0.2, emb_size), \n",
    "    optimizer = 'adam'\n",
    ")\n",
    "\n",
    "history = net.fit(\n",
    "    data_generator(x_train, y_train, batch_size, emb_size),\n",
    "    steps_per_epoch = steps_per_epoch,\n",
    "    epochs = epochs, \n",
    "    verbose = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.1509251743555069,\n",
       "  0.10886970907449722,\n",
       "  0.09359375387430191,\n",
       "  0.08184546232223511,\n",
       "  0.08012211322784424,\n",
       "  0.06901495903730392,\n",
       "  0.05989028885960579,\n",
       "  0.06119062751531601,\n",
       "  0.055470384657382965,\n",
       "  0.0481996051967144,\n",
       "  0.05493689700961113,\n",
       "  0.044487349689006805,\n",
       "  0.049256786704063416,\n",
       "  0.048073265701532364,\n",
       "  0.03907403349876404,\n",
       "  0.04024697095155716,\n",
       "  0.04073173925280571,\n",
       "  0.03780272603034973,\n",
       "  0.03703644499182701,\n",
       "  0.03154565393924713,\n",
       "  0.031303487718105316,\n",
       "  0.033992134034633636,\n",
       "  0.03340541571378708,\n",
       "  0.026767298579216003,\n",
       "  0.03024202212691307,\n",
       "  0.030400436371564865,\n",
       "  0.026095133274793625,\n",
       "  0.03145259618759155,\n",
       "  0.02768944576382637,\n",
       "  0.025003736838698387,\n",
       "  0.03377087041735649,\n",
       "  0.029419507831335068,\n",
       "  0.027707591652870178,\n",
       "  0.026433557271957397,\n",
       "  0.02624903991818428,\n",
       "  0.024844948202371597,\n",
       "  0.024600300937891006,\n",
       "  0.023549597710371017,\n",
       "  0.0222068652510643,\n",
       "  0.01800914853811264,\n",
       "  0.027073483914136887,\n",
       "  0.024587135761976242,\n",
       "  0.025127029046416283,\n",
       "  0.020433101803064346,\n",
       "  0.02515542134642601,\n",
       "  0.02004096657037735,\n",
       "  0.03187830001115799,\n",
       "  0.019629310816526413,\n",
       "  0.025687390938401222,\n",
       "  0.021523889154195786,\n",
       "  0.018630174919962883,\n",
       "  0.02118580788373947,\n",
       "  0.017635643482208252,\n",
       "  0.02151699922978878,\n",
       "  0.018937136977910995,\n",
       "  0.0203762948513031,\n",
       "  0.021872935816645622,\n",
       "  0.021789338439702988,\n",
       "  0.018289994448423386,\n",
       "  0.023020457476377487,\n",
       "  0.019741898402571678,\n",
       "  0.01650875434279442,\n",
       "  0.02323545143008232,\n",
       "  0.01384699810296297,\n",
       "  0.01658766344189644,\n",
       "  0.022404875606298447,\n",
       "  0.02058643102645874,\n",
       "  0.01639377325773239,\n",
       "  0.022918740287423134,\n",
       "  0.02599111758172512,\n",
       "  0.016936305910348892,\n",
       "  0.01840195059776306,\n",
       "  0.0192849300801754,\n",
       "  0.014492399990558624,\n",
       "  0.0176381915807724,\n",
       "  0.015514353290200233,\n",
       "  0.016043782234191895,\n",
       "  0.01952253468334675,\n",
       "  0.020120587199926376,\n",
       "  0.017376204952597618,\n",
       "  0.012117670848965645,\n",
       "  0.016395656391978264,\n",
       "  0.014421234838664532,\n",
       "  0.017245851457118988,\n",
       "  0.01720522716641426,\n",
       "  0.015367184765636921,\n",
       "  0.014970851130783558,\n",
       "  0.01882275938987732,\n",
       "  0.018431559205055237,\n",
       "  0.011907859705388546,\n",
       "  0.014377562329173088,\n",
       "  0.01481395959854126,\n",
       "  0.015397898852825165,\n",
       "  0.017781808972358704,\n",
       "  0.014407774433493614,\n",
       "  0.016526691615581512,\n",
       "  0.015107424929738045,\n",
       "  0.018682850524783134,\n",
       "  0.014646144583821297,\n",
       "  0.015191994607448578]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_anchor = embedding_model.predict(x_test[2].reshape(-1, n, n, 1))\n",
    "test_positive = embedding_model.predict(x_test[7].reshape(-1, n, n, 1))\n",
    "test_negative = embedding_model.predict(x_test[12].reshape(-1, n, n, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9560251]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(test_anchor, test_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.53414106]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(test_anchor, test_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, x_embed, _, y_embed = train_test_split(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    stratify = y_train,\n",
    "    test_size = 40,\n",
    "    random_state = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_embeddings = {}\n",
    "for i, y in enumerate(y_embed):\n",
    "    db_embeddings[str(y)] = embedding_model.predict(x_embed[i].reshape(-1, n, n, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"embeddings.pickle\", \"wb\") as f:\n",
    "    pickle.dump(db_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model.save(\"embedding_model.h5\")\n",
    "\n",
    "# mod =  tf.keras.models.load_model(\n",
    "#     \"embedding_model.h5\", \n",
    "#     custom_objects = {\n",
    "#         'loss': triplet_loss(0.2, 16)\n",
    "#     },\n",
    "#     compile = False\n",
    "# )\n",
    "# mod.compile( \n",
    "#     loss = triplet_loss(0.2, 16), \n",
    "#     optimizer = \"adam\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
