{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Siamese Network with Triplet Loss in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Understanding the Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from pca_plotter import PCAPlotter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Approach\n",
    "\n",
    "This appraoch is taken from the popular [FaceNet](https://arxiv.org/abs/1503.03832) paper.\n",
    "\n",
    "We have a CNN model called `EmbeddingModel`:\n",
    "\n",
    "![CNN](assets/CNN.png)\n",
    "\n",
    "We use three images for each training example:\n",
    "1. `person1_image1.jpg` (Anchor Example, represented below in green)\n",
    "2. `person1_image2.jpg` (Positive Example, in blue)\n",
    "3. `person2_image1.jpg` (Negative Example, in red).\n",
    "\n",
    "![Embeddings](assets/embeddings.png)\n",
    "\n",
    "\n",
    "## Siamese Network\n",
    "\n",
    "All the three images of an example pass through the model, and we get the three Embeddings: One for the Anchor Example, one for the Positive Example, and one for the Negative Example.\n",
    "\n",
    "![Siamese Network](assets/siamese.png)\n",
    "\n",
    "The three instances of the `EmbeddingModel` shown above are not different instances. It's the same, shared model instance - i.e. the parameters are shared, and are updated for all the three paths simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.reshape(x_train, (x_train.shape[0], 784))/255.\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], 784))/255.\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Plotting Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_triplets(examples):\n",
    "    plt.figure(figsize = (6, 2))\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, 1 + i)\n",
    "        plt.imshow(np.reshape(examples[i], (28, 28)), cmap = 'binary')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x144 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_triplets([x_train[0], x_train[1], x_train[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: A Batch of Triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(batch_size=256):\n",
    "    x_anchors = np.zeros((batch_size, 784))\n",
    "    x_positives = np.zeros((batch_size, 784))\n",
    "    x_negatives = np.zeros((batch_size, 784))\n",
    "    \n",
    "    for i in range(0, batch_size):\n",
    "        # We need to find an anchor, a positive example and a negative example\n",
    "        random_index = random.randint(0, x_train.shape[0] - 1)\n",
    "        x_anchor = x_train[random_index]\n",
    "        y = y_train[random_index]\n",
    "        \n",
    "        indices_for_pos = np.squeeze(np.where(y_train == y))\n",
    "        indices_for_neg = np.squeeze(np.where(y_train != y))\n",
    "           \n",
    "        x_positive = x_train[indices_for_pos[random.randint(0, len(indices_for_pos) - 1)]]\n",
    "        x_negative = x_train[indices_for_neg[random.randint(0, len(indices_for_neg) - 1)]]\n",
    "        \n",
    "        x_anchors[i] = x_anchor\n",
    "        x_positives[i] = x_positive\n",
    "        x_negatives[i] = x_negative\n",
    "        \n",
    "    return [x_anchors, x_positives, x_negatives]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x144 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "examples = create_batch(1)\n",
    "plot_triplets(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "=================================================================\n",
      "Total params: 54,400\n",
      "Trainable params: 54,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "emb_size = 64\n",
    "\n",
    "embedding_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation = 'relu', input_shape = (784, )),\n",
    "    tf.keras.layers.Dense(emb_size, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "embedding_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5378492  0.49610925 0.65808624 0.2957707  0.5261911  0.57732695\n",
      " 0.46608827 0.5748416  0.5036872  0.5275323  0.5996663  0.52261186\n",
      " 0.6470166  0.4397481  0.5545055  0.35242742 0.40251023 0.5478605\n",
      " 0.40971994 0.41027912 0.58143127 0.60826147 0.5104459  0.54782814\n",
      " 0.52134025 0.43671992 0.46465248 0.53494966 0.4280466  0.35624656\n",
      " 0.57493603 0.47889823 0.49555168 0.45989528 0.5723232  0.4588223\n",
      " 0.47502613 0.46867362 0.57186514 0.6247158  0.5079522  0.35987428\n",
      " 0.44063044 0.49460545 0.38840243 0.5109124  0.3986046  0.46874747\n",
      " 0.46954054 0.6094867  0.49095285 0.5692874  0.4136266  0.39693666\n",
      " 0.47423732 0.57343197 0.54208124 0.35101622 0.5735899  0.44108444\n",
      " 0.42506593 0.53118145 0.44728878 0.52832663]\n"
     ]
    }
   ],
   "source": [
    "example = np.expand_dims(x_train[0], axis = 0)\n",
    "example_emb = embedding_model.predict(example)[0]\n",
    "\n",
    "print(example_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6: Siamese Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 784)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 784)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 784)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 64)           54400       input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 192)          0           sequential[1][0]                 \n",
      "                                                                 sequential[2][0]                 \n",
      "                                                                 sequential[3][0]                 \n",
      "==================================================================================================\n",
      "Total params: 54,400\n",
      "Trainable params: 54,400\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_anchor = tf.keras.layers.Input(shape = (784,))\n",
    "input_positive = tf.keras.layers.Input(shape = (784,))\n",
    "input_negative = tf.keras.layers.Input(shape = (784,))\n",
    "\n",
    "embedding_anchor = embedding_model(input_anchor)\n",
    "embedding_positive = embedding_model(input_positive)\n",
    "embedding_negative = embedding_model(input_negative)\n",
    "\n",
    "output = tf.keras.layers.concatenate([embedding_anchor, embedding_positive, embedding_negative], axis=1)\n",
    "\n",
    "net = tf.keras.models.Model([input_anchor, input_positive, input_negative], output)\n",
    "net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7: Triplet Loss\n",
    "\n",
    "A loss function that tries to pull the Embeddings of Anchor and Positive Examples closer, and tries to push the Embeddings of Anchor and Negative Examples away from each other.\n",
    "\n",
    "Root mean square difference between Anchor and Positive examples in a batch of N images is:\n",
    "$\n",
    "\\begin{equation}\n",
    "d_p = \\sqrt{\\frac{\\sum_{i=0}^{N-1}(f(a_i) - f(p_i))^2}{N}}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "Root mean square difference between Anchor and Negative examples in a batch of N images is:\n",
    "$\n",
    "\\begin{equation}\n",
    "d_n = \\sqrt{\\frac{\\sum_{i=0}^{N-1}(f(a_i) - f(n_i))^2}{N}}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "For each example, we want:\n",
    "$\n",
    "\\begin{equation}\n",
    "d_p \\leq d_n\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "Therefore,\n",
    "$\n",
    "\\begin{equation}\n",
    "d_p - d_n \\leq 0\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "This condition is quite easily satisfied during the training.\n",
    "\n",
    "We will make it non-trivial by adding a margin (alpha):\n",
    "$\n",
    "\\begin{equation}\n",
    "d_p - d_n + \\alpha \\leq 0\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "Given the condition above, the Triplet Loss L is defined as:\n",
    "$\n",
    "\\begin{equation}\n",
    "L = max(d_p - d_n + \\alpha, 0)\n",
    "\\end{equation}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(alpha, emb_size):\n",
    "    \n",
    "    def loss(y_true, y_pred):\n",
    "        anchor, positive, negative = y_pred[:,:emb_size], y_pred[:,emb_size:2*emb_size], y_pred[:,2*emb_size:]\n",
    "        positive_dist = tf.reduce_mean(tf.square(anchor - positive), axis = 1)\n",
    "        negative_dist = tf.reduce_mean(tf.square(anchor - negative), axis = 1)\n",
    "        \n",
    "        return tf.maximum(positive_dist - negative_dist + alpha, 0.)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8: Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(batch_size, emb_size):\n",
    "    while True:\n",
    "        x = create_batch(batch_size)\n",
    "        y = np.zeros((batch_size, 3*emb_size))\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 9: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-a619f24d7670>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0membedding_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         )\n\u001b[1;32m     22\u001b[0m     ]\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m           distribution_strategy=strategy)\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    548\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    604\u001b[0m       \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m       \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m       use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    607\u001b[0m   \u001b[0;31m# As a fallback for the data type that does not work with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m   \u001b[0;31m# _standardize_user_data, use the _prepare_model_with_inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m     self._dataset = dataset_ops.DatasetV2.from_generator(\n\u001b[0;32m--> 566\u001b[0;31m         reassemble, nested_dtypes, output_shapes=nested_shape)\n\u001b[0m\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_generator\u001b[0;34m(generator, output_types, output_shapes, args)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m       output_shapes = nest.map_structure_up_to(\n\u001b[0;32m--> 540\u001b[0;31m           output_types, tensor_shape.as_shape, output_shapes)\n\u001b[0m\u001b[1;32m    541\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m       \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/util/nest.py\u001b[0m in \u001b[0;36mmap_structure_up_to\u001b[0;34m(shallow_tree, func, *inputs)\u001b[0m\n\u001b[1;32m    469\u001b[0m                          for input_tree in inputs]\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m   \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensors\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mall_flattened_up_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshallow_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    469\u001b[0m                          for input_tree in inputs]\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m   \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensors\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mall_flattened_up_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshallow_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mas_shape\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m   1214\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dims)\u001b[0m\n\u001b[1;32m    774\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;31m# Got a list of dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdims_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    774\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;31m# Got a list of dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdims_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mas_dimension\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    716\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    191\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot convert %s to Dimension\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m       if (not isinstance(value, compat.bytes_or_text_types) and\n\u001b[1;32m    195\u001b[0m           self._value != value):\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'tuple'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "epochs = 10\n",
    "steps_per_epoch = int(x_train.shape[0]/batch_size)\n",
    "\n",
    "net.compile(\n",
    "    loss = triplet_loss(0.2, emb_size), \n",
    "    optimizer = 'adam'\n",
    ")\n",
    "\n",
    "_ = net.fit_generator(\n",
    "    data_generator(batch_size, emb_size),\n",
    "    steps_per_epoch = steps_per_epoch,\n",
    "    epochs = epochs, \n",
    "    verbose = False,\n",
    "    callbacks = [\n",
    "        PCAPlotter(\n",
    "            plt, \n",
    "            embedding_model,\n",
    "            x_test[:1000], \n",
    "            y_test[:1000]\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data_generator(batch_size, emb_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
